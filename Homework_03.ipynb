{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "coursera": {
      "course_slug": "neural-networks-deep-learning",
      "graded_item_id": "XaIWT",
      "launcher_item_id": "zAgPl"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzo37yE6ejuA"
      },
      "source": [
        "# Convolution Neural Network Homework\n",
        "\n",
        "This is the 3rd homework assignment for CAP 4630 and we will go through some primary operations for image processsing and implement one of the earilest representative convolution neural network - LeNet-5 . \\\n",
        "You will use **\"Tasks\"** and **\"Hints\"** to finish the work. **(Total 100 Points)** \\\n",
        "For section 1, when you implement covolution and maxpooling, you are **not** allowed to use built-in functions in Machine Learning libaries such as Scikit-learn Keras, Tensorflow, Pytorch; but you are encouraged to employ Keras for second section.\n",
        "\n",
        "**Task Overview:**\n",
        "- Basic operations for Digital Image Processing (DIP)\n",
        "- LeNet-5 (Google Colab is recommended for implementation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLKjpQvaejuF"
      },
      "source": [
        "## 1 - Basic Image Processing ##\n",
        "### 1.1 Data Preparation \n",
        "\n",
        "Import packages and prepare image data as an array for image processing. **(5 Points)**\n",
        "\n",
        "**Tasks:**\n",
        "1. Import numpy and rename it to np.\n",
        "2. Import imageio and call imread to convert image to an array.\n",
        "3. **DISPLAY** the image in the output box before image-array conversion.\n",
        "4. **PRINT OUT** the size of the array\n",
        "5. **PRINT OUT** the numeric matrix form of image, i.e. the obtained array after image-array conversion.\n",
        "\n",
        "References:\n",
        "- [numpy](www.numpy.org) is the fundamental package for scientific computing with Python.\n",
        "- [imageio](https://imageio.github.io/) is a python library for basic image reading and writing.\n",
        "\n",
        "**Hints:**\n",
        "1. Image data is under current directory, i.e., \"./image.jpg\".\n",
        "2. You may consider importing \"display\" and \"Image\" from IPython.display for image display."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvITLRocejuG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e978912b-96e9-44ce-fe4e-95f6d109f2f2"
      },
      "source": [
        "# Import useful libraries\n",
        "import numpy as np\n",
        "import imageio as iio\n",
        "from IPython.display import display, Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "IMG_PATH = './image.jpg'\n",
        "\n",
        "# Display original image\n",
        "def load_image(IMG_PATH):    \n",
        "    from google.colab import files\n",
        "    uploaded = files.upload()\n",
        "    return Image(IMG_PATH)\n",
        "     \n",
        "\n",
        "# # Convert image to array, print out the shape of array, and print out the entire array\n",
        "def img_to_array(IMG_PATH):    \n",
        "    img_matrix = iio.imread(IMG_PATH)\n",
        "    return img_matrix.astype(np.float32)\n",
        "\n",
        "image_matrix = img_to_array(IMG_PATH)\n",
        "\n",
        "display(load_image(IMG_PATH))\n",
        "print('Shape:', image_matrix.shape, '\\nSize:', image_matrix.size)\n",
        "print('Numeric matrix:\\n', image_matrix)\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-1441822f-d6c4-43ef-a56d-e533429b0d58\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-1441822f-d6c4-43ef-a56d-e533429b0d58\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAeAB4BAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/AGfD7w+l5qkV5qJkjt0MkYRpyfOYrjAVuvU8/wCz2r0m+sPC+mSwQXdlpqSybvIW4UHcR7t1PSuW1C8srPUrSG2s7W3s7h1jP2dRxIwc5wD0wnp3ry/xPpk9trEvnFWSb95G6nhl6fn7V32j6v4a0TVLGxaZJJ7eSRLWSU7/ACnJbJLd2O5h09O9ReLZdP17UJW1G3t5ry3BjiEhYqBk9SMY/I1zVzbhrGyhEvlxI7vhOhbAGM9sA4H4+tRajqtqLSNLyza7dHKiQNsVR6A/j09qs+KPBN/J4mfUtMtFkjlIkIjYfI/8R7dxu71KY7rTJJluiVuZFAkRlyxzz1xx65rJn1BLSB/tYzGASsbOV+bGMnBrkbjUlN60lrM6llAYglc+/Wvo9VX7sih0Y4ZT3Fc78QtDtoNEkvLdSkkEzQNlidygDB9u3HT8ufD5IZb+6ZXlPl5yQf8APNJZabDdyyCL5AvQsM1//9k=\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape: (30, 30) \n",
            "Size: 900\n",
            "Numeric matrix:\n",
            " [[ 98.  89.  78. 112.  90.  93. 158. 112. 122. 117. 107. 121.  90.  75.\n",
            "   67.  93.  73.  67.  96. 152. 167. 151. 166. 154. 134. 105.  53.  51.\n",
            "   55.  44.]\n",
            " [ 34.  31.  51.  38.  16.  50. 195. 161. 138. 173. 200. 209. 194. 206.\n",
            "  222. 237. 223. 195. 177. 185. 167. 160. 166. 169. 136.  98.  49.  54.\n",
            "   55.  35.]\n",
            " [ 78.  84.  94.  71.  81.  69. 190. 204. 233. 240. 249. 234. 232. 241.\n",
            "  250. 235. 235. 236. 224. 194. 169. 181. 170. 162. 121.  95.  55.  49.\n",
            "   52.  47.]\n",
            " [ 79.  58.  90. 127. 103. 137. 253. 244. 254. 222. 223. 219. 235. 228.\n",
            "  232. 206. 201. 215. 201. 106. 105. 137. 169. 173. 124.  81.  45.  49.\n",
            "   59.  53.]\n",
            " [ 31.  75.  44.  81.  89. 191. 242. 255. 228. 208. 231. 233. 236. 222.\n",
            "  227. 199. 177. 192. 208. 104. 137. 109. 170. 194. 159.  88.  44.  54.\n",
            "   63.  46.]\n",
            " [ 85.  66.  51. 145. 129. 225. 239. 206. 242. 233. 247. 231. 210. 218.\n",
            "  232. 202. 190. 173. 181.  95. 146.  39. 132. 199. 178. 145. 103.  53.\n",
            "   44.  65.]\n",
            " [ 81.  60.  76. 178. 216. 214. 209. 230. 240. 229. 230. 211. 157. 176.\n",
            "  214. 218. 185. 186. 198. 154. 203.  33. 100. 157. 157. 179. 162.  72.\n",
            "   39.  78.]\n",
            " [104.  89. 152. 176. 168. 117. 182. 226. 242. 235. 238. 207.  82.  50.\n",
            "   94. 145. 136. 157. 148. 119. 175.  30. 111. 165. 136. 155. 168. 110.\n",
            "   60.  56.]\n",
            " [ 95.  39. 181. 160.  91.  13. 180. 237. 240. 242. 221. 102.  24.   7.\n",
            "   19.  12.  38.  95. 101. 142. 155.  30.  89.  91. 107. 145. 161. 113.\n",
            "   60.  53.]\n",
            " [ 18.  54. 186. 144. 147.  99. 248. 238. 235. 227. 210. 115.  62.  57.\n",
            "   74.  72.  85.  82.  91. 182.  83.  56.  68.  59. 129. 113. 133. 129.\n",
            "  116.  67.]\n",
            " [ 84.  79. 201. 132. 185. 229. 217. 206. 210. 188. 188. 155. 160. 169.\n",
            "  164. 141. 111.  70. 126. 154.  49.  40.  69.  73. 104.  85. 120. 132.\n",
            "  158. 125.]\n",
            " [201. 161. 135. 153. 174. 219. 204. 197. 209. 178. 177. 146. 138. 124.\n",
            "  114. 101.  90.  85. 144. 105.  81.  53.  83. 104.  74.  80. 122. 141.\n",
            "  152. 133.]\n",
            " [162. 150. 147. 212. 176. 180. 190. 187. 161. 107. 114. 140. 191. 178.\n",
            "  114.  49.  74. 117. 112. 118.  99. 114.  93.  98.  96.  90. 104. 139.\n",
            "  102.  67.]\n",
            " [184. 138. 167. 132.  90. 118. 183. 214. 193. 150. 148. 132. 137. 130.\n",
            "  135. 141.  96. 127. 108. 140.  75. 111. 113.  96. 115. 101.  75.  89.\n",
            "   42.  43.]\n",
            " [172. 127. 105.  83.  61.  34. 134. 149. 188. 134. 119.  97. 106. 106.\n",
            "  115. 120. 109. 107. 123. 139.  70.  77. 110. 103. 102.  91.  67.  68.\n",
            "   42.  61.]\n",
            " [139.  39.  59.  55.  36.  15.  27. 128. 189. 123.  98.  79. 100. 105.\n",
            "  105.  97.  95.  86. 105. 148.  94.  95.  78.  92.  93.  67.  78. 110.\n",
            "   94.  51.]\n",
            " [162.  27.  19.  24.   2.  34.  19. 104. 153.  91.  88.  71. 100. 151.\n",
            "  122. 118.  98.  96.  92. 143. 136.  86.  92.  79.  83.  64.  35.  31.\n",
            "   61.  87.]\n",
            " [186. 114.  41.  35.  15.  20.  37.  75. 101.  92.  91.  86. 122. 172.\n",
            "  147.  97.  85.  96.  86. 104.  88.  61.  80.  64.  68.  65.  37.  12.\n",
            "   21.  38.]\n",
            " [183. 164.  24.  17.  41.  42.  75.  49.  67.  80.  74.  86.  98. 104.\n",
            "  116.  99.  81. 101.  90.  94.  97. 116. 149. 125.  69.  83.  64.  30.\n",
            "   28.  42.]\n",
            " [172. 207.  94.  39.  34.  18.  50.  52.  82.  95.  92.  99.  97.  86.\n",
            "  104. 120.  82.  88.  69.  66.  82. 112. 126.  84.  52.  66.  53.  25.\n",
            "   20.  26.]\n",
            " [159. 164. 106.  37.  51.  56.  38.  56.  62.  87. 101.  90. 108. 125.\n",
            "   99.  84.  94.  90.  78.  79.  86.  97.  89.  48.  79.  77.  59.  41.\n",
            "   34.  27.]\n",
            " [179. 181. 191. 103.  67.  52.  28.  77.  92. 109. 121. 113. 129. 129.\n",
            "   93.  93.  65.  66.  77.  86.  83.  79.  71.  52.  55.  46.  30.  24.\n",
            "   26.  19.]\n",
            " [198. 178. 191. 158. 154. 149. 142. 139. 144. 144. 109. 102. 103.  73.\n",
            "   72.  98.  89.  84.  94.  92.  76.  65.  51.  40.  19.  21.  12.   9.\n",
            "   19.  25.]\n",
            " [173. 168. 176. 182. 184. 172. 208. 171. 198. 207. 118.  91. 101.  88.\n",
            "  107.  93. 109.  92.  87.  72.  55.  47.  26.   6.   9.  21.  14.   1.\n",
            "    9.  19.]\n",
            " [180. 180. 176. 177. 187. 189. 165. 134. 130. 188. 137. 113.  70. 107.\n",
            "   79.  61.  81.  61.  47.  49.  48.  30.  10.   0.   4.   5.   5.   5.\n",
            "    4.   7.]\n",
            " [176. 177. 176. 178. 186. 190. 183. 172. 160. 151. 124. 134. 109.  74.\n",
            "   58.  64.  56.  48.  42.  37.  23.   8.   8.  17.   9.   9.  11.  14.\n",
            "   18.  23.]\n",
            " [175. 174. 176. 180. 181. 185. 195. 207. 179. 208. 217. 168. 122.  92.\n",
            "   94.  48.  52.  34.  18.  14.  12.  10.  14.  21.  33.  28.  23.  23.\n",
            "   26.  28.]\n",
            " [179. 174. 177. 182. 178. 175. 189. 210. 216. 220. 204. 246. 134. 106.\n",
            "   65.  57.  55.  34.  14.   9.  15.  21.  27.  32.  54.  43.  31.  29.\n",
            "   34.  36.]\n",
            " [183. 176. 177. 185. 182. 172. 179. 196. 255. 153. 188. 221. 225.  84.\n",
            "   82.  51.  42.  39.  33.  25.  19.  23.  41.  59.  49.  37.  27.  31.\n",
            "   47.  59.]\n",
            " [184. 177. 179. 188. 186. 177. 177. 186. 242. 187. 172. 247. 199. 148.\n",
            "   91.  54.  40.  35.  30.  29.  34.  44.  55.  63.  34.  24.  18.  28.\n",
            "   51.  71.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EY5djq-tejuI"
      },
      "source": [
        "### 1.2 Implementation of Convolution Filter\n",
        "\n",
        "Process the obtained array from the image with convolution operation. **(20 Points)**\n",
        "\n",
        "**Tasks:**\n",
        "1. Prepare a 3X3 Laplacian kernel (aka Laplacial filter) with array as convolution filter.\n",
        "2. Conduct convolution on image with prepared kernel.\n",
        "3. **PRINT OUT** convolution result for first ten rows.\n",
        "4. **PRINT OUT** the shape of the convolution result.\n",
        "5. **DISPLAY** convolution result as image with matplotlib. (Don't worry about the value <0 or >255. Scaling process will be conducted in imshow function to make sure valid display.)\n",
        "\n",
        "\n",
        "**Hints:**\n",
        "1. Laplacian kernel is widely used for edge detection. Its form is shown below:\n",
        "\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=15bP8KCwHLtglJ-WXV4wolm4m46mCp3HL)\n",
        "\n",
        "2. You may consider the following steps for this implementation.\\\n",
        "    2.1 Extract all centriods of processing region for each convolution operation.\\\n",
        "    2.2 According to each centroid, locate all indices of the elements within the local region for each convolution operation.\\\n",
        "    2.3 Given obtained indices, locate pixel values (i.e. our obtained array elements) and conduct element-wise product between pixel and kernel values.\\\n",
        "    2.4 Sum element-wise product results and assign the value to convolution result at corresponding location.\\\n",
        "    **Note: we did not conduct padding for processed array, and thus, convolution result will become smaller than original array. You may think about the reason.**\n",
        "3. Validation for first 5X5 array (from upper-left corner), i.e., filtered_results[0:5,0:5]. The example figure is below.\n",
        "\n",
        "[[ 134.   37.   98.  195.  173.]\\\n",
        " [ -75.  -80.   56.  -65.  182.]\\\n",
        " [  96.  -37. -163.   22.   68.]\\\n",
        " [-101.  121.   81.  148.  -71.]\\\n",
        " [   7.  127. -141.  159. -127.]]\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=18Iis1mJsvEaojZ7O3f3soE152Szwy8_Z)\n",
        "\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cnc8BNW9ejuJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 821
        },
        "outputId": "74ed4d06-4d51-477f-9faa-ecd714c546b6"
      },
      "source": [
        "######## Convolution with Laplacian Filter ##################\n",
        "Laplacian = np.array([[0, 1, 0],\n",
        "                      [1, -4, 1],\n",
        "                      [0, 1, 0]])\n",
        "\n",
        "def convolution(matrix, filter, stride):\n",
        "    img_x, img_y = matrix.shape\n",
        "    filter_x, filter_y = filter.shape\n",
        "    output_shape = (1 + (img_x - filter_x) // stride, 1 + (img_y - filter_y) // stride, filter_x, filter_y)\n",
        "    output_stride = (matrix.strides[0] * stride, matrix.strides[1] * stride, matrix.strides[0], matrix.strides[1])\n",
        "    windows = np.lib.stride_tricks.as_strided(matrix, shape=output_shape, strides=output_stride)\n",
        "    return np.tensordot(windows, filter, axes=((2, 3), (0, 1)))\n",
        "\n",
        "\n",
        "filtered_results = convolution(image_matrix, Laplacian, stride=1)\n",
        "print('Result matrix (first 10 rows):\\n', filtered_results[:10,:])\n",
        "print('Result shape:',filtered_results.shape)\n",
        "imgplot = plt.imshow(filtered_results)\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result matrix (first 10 rows):\n",
            " [[ 134.   37.   98.  195.  173. -221.    5.  137.    3.  -62.  -87.  -39.\n",
            "   -92. -128. -175. -152.  -77.   -8.  -50.   13.   25.    1.  -58.  -22.\n",
            "    -7.   64.  -12.  -24.]\n",
            " [ -75.  -80.   56.  -65.  182.  -39.   12.  -96.  -83.  -99.  -27.  -24.\n",
            "   -48.  -70.  -12.  -45.  -75.  -88.  -92.  -29.  -88.   -2.  -15.   33.\n",
            "   -25.   18.   14.    2.]\n",
            " [  96.  -37. -163.   22.   68. -199.  -10.  -89.   37.   29.   49.  -25.\n",
            "    18.  -17.   43.   29.  -30.  -51.  180.  129.   16.  -26.  -43.   38.\n",
            "    28.   49.   11.  -19.]\n",
            " [-101.  121.   81.  148.  -71.  -30. -100.   47.   82.  -13.  -15.  -44.\n",
            "    21.  -23.   16.   74.    5. -154.  130.  -84.   47.  -76.  -75.  -52.\n",
            "    77.  114.   -7.  -49.]\n",
            " [   7.  127. -141.  159. -127.  -74.  142.  -61.   -6.  -63.  -23.    2.\n",
            "   -32.  -67.   31.  -23.   57.  -50.  205. -110.  264.  -20. -135.  -52.\n",
            "   -32.   -8.   61.   44.]\n",
            " [  72.  137.  -99. -175.  -89.   29.  -39.  -17.   22.    5.  -19.   51.\n",
            "   -65. -136. -126.  -10.  -31. -123.   -1. -304.  240.   33.   -7.   22.\n",
            "   -97. -126.   76.   98.]\n",
            " [  -1.  -86.  -46.  -72.  109.    4.  -13.  -27.   11.  -59. -195.  110.\n",
            "   159.   52. -120.  -19.  -63.  -17.  143. -193.  229.  -60. -165.   40.\n",
            "     8.  -84.  -27.   25.]\n",
            " [ 263. -187.  -48.  124.  435.  -40.  -64.   -4.  -45.  -92.  159.  157.\n",
            "   122.  111.  226.  176.   -2.   72.  -11. -190.  210.  -56.   56.   73.\n",
            "   -44.  -85.    8.  102.]\n",
            " [ 106. -164.   49.  -69.  241. -258.  -26.  -25.  -33.  -89.   69.  108.\n",
            "    84.   16.   24.  -37.   13.  127. -258.  110.   -3.    1.  125. -133.\n",
            "    40.   -9.  -22.  -50.]\n",
            " [ 184. -272.  155.  -58. -196.   19.   38.   -2.   51.  -22.  -11. -116.\n",
            "  -171. -158. -116.  -58.  124.  -45. -154.  162.   67.  -12.   44.  -55.\n",
            "    77.   -8.   20. -107.]]\n",
            "Result shape: (28, 28)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMN_XE1xejuJ"
      },
      "source": [
        "### 1.3 Modification on Convolution Scheme\n",
        "\n",
        "Conduct the convolution with the same filter as above, but change the stride to 2. **(5 Points)**\n",
        "\n",
        "**Tasks:**\n",
        "1. Modify the convolution process with stride=2\n",
        "2. **PRINT OUT** convolution result for first ten rows.\n",
        "3. **PRINT OUT** the shape of the convolution result.\n",
        "4. **DISPLAY** convolution result as image with matplotlib.(Don't worry about the value <0 or >255. Scaling process will be conducted in imshow function to make sure valid display.)\n",
        "\n",
        "**Hints:**\n",
        "1. You may just reduce the centroid pool according to stride=2, and then, follow the same convolution process above.\n",
        "    **Note: After increase of stride, the size of convolution result is further shrinked. You may think about the reason.**\n",
        "2. Validation for first 5X5 array (from upper-left corner), i.e., filtered_results[0:5,0:5]. The example figure is below.\n",
        "\n",
        "[[ 134.   98.  173.    5.    3.]\\\n",
        " [  96. -163.   68.  -10.   37.]\\\n",
        " [   7. -141. -127.  142.   -6.]\\\n",
        " [  -1.  -46.  109.  -13.   11.]\\\n",
        " [ 106.   49.  241.  -26.  -33.]]\n",
        " \n",
        " \n",
        "![](https://drive.google.com/uc?export=view&id=1UPdXt5cY1umImu2chaQLfWAnqDEpFOGV)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pyFLwZxYejuJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 647
        },
        "outputId": "0da768f4-df34-461c-fc5f-ef2ec6a548a3"
      },
      "source": [
        "######## Convolution with Laplacian Filter and the setting of stride=2 ##################\n",
        "filtered_results = convolution(image_matrix, Laplacian, stride=2)\n",
        "print('Result matrix (first 10 rows):\\n', filtered_results[:10,:])\n",
        "print('Result shape:', filtered_results.shape)\n",
        "imgplot = plt.imshow(filtered_results)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result matrix (first 10 rows):\n",
            " [[ 134.   98.  173.    5.    3.  -87.  -92. -175.  -77.  -50.   25.  -58.\n",
            "    -7.  -12.]\n",
            " [  96. -163.   68.  -10.   37.   49.   18.   43.  -30.  180.   16.  -43.\n",
            "    28.   11.]\n",
            " [   7. -141. -127.  142.   -6.  -23.  -32.   31.   57.  205.  264. -135.\n",
            "   -32.   61.]\n",
            " [  -1.  -46.  109.  -13.   11. -195.  159. -120.  -63.  143.  229. -165.\n",
            "     8.  -27.]\n",
            " [ 106.   49.  241.  -26.  -33.   69.   84.   24.   13. -258.   -3.  125.\n",
            "    40.  -22.]\n",
            " [ -79.   41.  -89.   18.  -31.   26.  103.  -10.   81.   77.  106.  -88.\n",
            "    51.  -19.]\n",
            " [  76.   24.   15. -144.  -18.   -6.   36. -164.  -80. -120.  -65.   45.\n",
            "   -33.  -32.]\n",
            " [ 196.  -18.   71.  -43.   20.   50.   42.   50.   59. -111.  -45.  -15.\n",
            "    58. -169.]\n",
            " [ -38.  -43.   48.   -9.   -5.   26. -164.   61.  -16.   -5.  126.   96.\n",
            "    -8.   71.]\n",
            " [-234.   26.  110.   29.  -39.  -31.   86. -111.  -10.   60.  -27.   15.\n",
            "     1.   44.]]\n",
            "Result shape: (14, 14)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2PT6NpSejuL"
      },
      "source": [
        "### 1.4 Implementation of MaxPooling\n",
        "\n",
        "Process the obtained array from the image with MaxPooling operation. **(15 Points)**\n",
        "\n",
        "**Tasks:**\n",
        "1. Prepare a 2X2 pooling mask.\n",
        "2. Conduct max pooing on image with prepared mask.\n",
        "3. **PRINT OUT** convolution result for first ten rows.\n",
        "4. **PRINT OUT** the shape of the convolution result.\n",
        "5. **DISPLAY** convolution result as image with matplotlib.(Don't worry about the value <0 or >255. Scaling process will be conducted in imshow function to make sure valid display.)\n",
        "\n",
        "**Hints:**\n",
        "1. You may just modify the centroid pool to top-left corner pool, and then, follow the same strategy above.\\\n",
        "    **Note: After the pooling, the size of the array is shrinked. You may think about the reason.**\n",
        "2. Validation for first 5X5 array (from upper-left corner), i.e., pooled_results[0:5,0:5].The example figure is below.\n",
        "\n",
        "[[ 98. 112.  93. 195. 173.]\\\n",
        " [ 84. 127. 137. 253. 254.]\\\n",
        " [ 85. 145. 225. 255. 242.]\\\n",
        " [104. 178. 216. 230. 242.]\\\n",
        " [ 95. 186. 147. 248. 242.]]\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1a18IWjrN0xHcp7bSNuj8kUM4JFFj3ebd)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOV72Sp9ejuL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 647
        },
        "outputId": "dc07e8f3-9124-4d60-8895-9c99326544ad"
      },
      "source": [
        "######## MaxPooling with the setting of 2X2 ##################\n",
        "def pooling(image):\n",
        "    img_x, img_y = image.shape\n",
        "    filter_x, filter_y = 2,2\n",
        "\n",
        "    filter_x, filter_y = 2, 2 \n",
        "    ny = img_x // filter_y\n",
        "    nx = img_y // filter_x\n",
        "    image_pad = image[:ny*filter_y, :nx*filter_x, ...]\n",
        "    output_shape = (ny,filter_y,nx,filter_x) + image.shape[2:]\n",
        "    pooled_results = np.nanmax(image_pad.reshape(output_shape), axis=(1,3))\n",
        "    return pooled_results\n",
        "\n",
        "pooled_results = pooling(image_matrix)\n",
        "print('Result matrix (first 10 rows):\\n', pooled_results[:10,:])\n",
        "print('Result shape:',pooled_results.shape)\n",
        "imgplot = plt.imshow(pooled_results)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result matrix (first 10 rows):\n",
            " [[ 98. 112.  93. 195. 173. 209. 206. 237. 223. 185. 167. 169. 136.  54.\n",
            "   55.]\n",
            " [ 84. 127. 137. 253. 254. 249. 241. 250. 236. 224. 181. 173. 124.  55.\n",
            "   59.]\n",
            " [ 85. 145. 225. 255. 242. 247. 236. 232. 192. 208. 146. 199. 178. 103.\n",
            "   65.]\n",
            " [104. 178. 216. 230. 242. 238. 176. 218. 186. 198. 203. 165. 179. 168.\n",
            "   78.]\n",
            " [ 95. 186. 147. 248. 242. 221.  62.  74.  95. 182. 155.  91. 145. 161.\n",
            "  116.]\n",
            " [201. 201. 229. 217. 210. 188. 169. 164. 111. 154.  81. 104. 104. 141.\n",
            "  158.]\n",
            " [184. 212. 180. 214. 193. 148. 191. 141. 127. 140. 114. 113. 115. 139.\n",
            "  102.]\n",
            " [172. 105.  61. 149. 189. 119. 106. 120. 109. 148.  95. 110. 102. 110.\n",
            "   94.]\n",
            " [186.  41.  34. 104. 153.  91. 172. 147.  98. 143. 136.  92.  83.  37.\n",
            "   87.]\n",
            " [207.  94.  42.  75.  95.  99. 104. 120. 101.  94. 116. 149.  83.  64.\n",
            "   42.]]\n",
            "Result shape: (15, 15)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCxlJYlqejuM"
      },
      "source": [
        "## 2 - Convolution Neural Network ##\n",
        " \n",
        "In this section, we will use LeNet-5 (LeCun et al., 1998), one of representative deep nueral networks, to solve a  classification problem with Fashion MNIST benchmark.\n",
        "\n",
        "### 2.1 Library Preparation\n",
        "\n",
        "Import useful deep learning packages. \n",
        "\n",
        "**Tasks:**\n",
        "1. Import numpy and rename it to np.\n",
        "2. Import keras from tensorflow.\n",
        "3. Import layers from tensorflow.keras\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5enuEKHejuM"
      },
      "source": [
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUs3cBXXejuN"
      },
      "source": [
        "### 2.2 Training Data Preparation\n",
        "\n",
        "Import useful packages and prepare Fashion MNIST data. **(20 Points)**\n",
        "\n",
        "**Tasks:**\n",
        "1. Download Fashion MNIST data and split it with keras and prepare training/test data sets.\n",
        "2. Preprocess training/test data with normalization, dimension extension, and zero padding (for LeNet-5 configuration).\n",
        "3. Preprocess label data to binary class matrices.\n",
        "4. **PRINT OUT** first image in training set and its correponding label index\n",
        "5. **PRINT OUT** the shape of total training data, the number of training samples, and the number of test samples.\n",
        "\n",
        "**Hints**\n",
        "1. You may consider load function from the reference link. https://keras.io/api/datasets/ It provides dataloader function which can tackle downloading and data splitting automatically.\n",
        "2. For label preprocessing, you may consider **keras.utils.to_categorical** to convert class vectors to binary class matrices. This conversion makes sure the label can match the format of prediction output from neural network.\n",
        "3. For image display, consider showing the image and label **before dimension expansion and label preprocessing**.\n",
        "4. You may consider MNIST processing shown in class as an example.\n",
        "\n",
        "**References**\n",
        "- Fashion MNIST https://github.com/zalandoresearch/fashion-mnist\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjyfYiGrejuN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "outputId": "b0bbf9f9-3aab-47fb-f21c-4dfec8d8f358"
      },
      "source": [
        "# # ---------------the data, split between train and test sets with keras.datasets---------------\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
        "\n",
        "\n",
        "num_classes = len(np.unique(y_train))\n",
        "input_shape = (32, 32, 1) ## think about the reason\n",
        "\n",
        "# # ---------------Image Normalization (Scaling to [0, 1])---------------\n",
        "x_train = x_train.astype(np.float32) / 255.0\n",
        "x_test = x_test.astype(np.float32) / 255.0\n",
        "\n",
        "\n",
        "# Print out first image and its correponding label index\n",
        "plt.imshow(x_train[0])# , cmap='gray')\n",
        "print('First image( Label index', y_train[0], '):')\n",
        "plt.show()\n",
        "# # ---------------Dimension expansion to ensure that images have shape (28, 28, 1)---------------\n",
        "x_train = x_train.reshape((x_train.shape[0], 28, 28, 1))\n",
        "x_test = x_test.reshape((x_test.shape[0], 28, 28, 1))\n",
        "\n",
        "# # ---------------Conduct padding on training/test images to (32, 32, 1) for LeNet-5---------------\n",
        "x_train = np.pad(x_train, ((0,0),(2,2),(2,2),(0,0)), 'constant')\n",
        "x_test = np.pad(x_test, ((0,0),(2,2),(2,2),(0,0)), 'constant')\n",
        "\n",
        "# # ---------------Print out the training/test data shapes and the numbers of training/test samples---------------\n",
        "print(\"x_train shape:\", x_train.shape)\n",
        "print(x_train.shape[0], \"train samples\")\n",
        "print(x_test.shape[0], \"test samples\")\n",
        "\n",
        "# # ---------------convert label vectors to binary class matrices for training/test labels---------------\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First image( Label index 9 ):\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train shape: (60000, 32, 32, 1)\n",
            "60000 train samples\n",
            "10000 test samples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sw4GqzZmejuN"
      },
      "source": [
        "### 2.3 LeNet-5 \n",
        "\n",
        "Construct LeNet-5 as learning model for Fashion MNIST classification task. **(15 Points)**\n",
        "\n",
        "**Tasks:**\n",
        "1. Build up LeNet-5 with keras.Sequential\n",
        "2. Set the regularizer to l2 and regularizer lambda is **4e-5**.\n",
        "2. **PRINT OUT** the model summary.\n",
        "\n",
        "**Hints:**\n",
        "1. You may consider the convolution neural network shown in class as an example.\n",
        "2. The structure of LeNet-5 is listed below. Try to map each step to related processing operation. You can also search some materials to faciliate implementation. \n",
        "3. Some architecture settings are listed below. \n",
        "    - The kernel size for 2D convolution filter is **5 X 5**. You may think about the reason by calculation.\n",
        "    - Regularizer is set to L2 regularizer with **kernel_regularizer=regularizers.l2(4e-5)**.\n",
        "    - We change tanh activation to **\"relu\"** activation here. Please use **activation=\"relu\"** for implementation.\n",
        "    - We use MaxPooling instead of original AveragePooling. Please use \"**MaxPooling2D(pool_size=(2, 2))**\" for implementation.\n",
        "    - Please use **Flatten** to onvert 2D convolution layer to 1D fully connected layer.\n",
        "    - Gaussian connections are replaced with Softmax, and thus, the outputs are activated by Softmax function based on the number of classes.\n",
        "\n",
        "4. Validation result:\n",
        "    - Total params: 61,706\n",
        "    - Trainable params: 61,706\n",
        "    - Non-trainable params: 0\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1Ks9RasENa0KiYRi2vwfJ_BQxkLB-x49R)\n",
        "\n",
        "\n",
        "**References:**\n",
        "- http://yann.lecun.com/exdb/lenet/\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c97heiiSejuN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a5776f2-5899-4a04-8c51-d51912e3b970"
      },
      "source": [
        "### Construct LeNet-5\n",
        "\n",
        "from keras import regularizers\n",
        "keras.backend.clear_session()\n",
        "\n",
        "model = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=input_shape),\n",
        "        layers.Conv2D(6, kernel_size=(5,5), kernel_regularizer=regularizers.l2(4e-5), activation='relu'), #C1\n",
        "        layers.MaxPooling2D(pool_size=(2,2)), # S2 Subsampling\n",
        "        layers.Conv2D(16, kernel_size=(5,5), kernel_regularizer=regularizers.l2(4e-5), activation='relu'), #C3\n",
        "        layers.MaxPooling2D(pool_size=(2,2)), # S4 Subsampling\n",
        "        layers.Flatten(), # Convert 2D convolution layer to 1D fully connected layer\n",
        "        layers.Dense(120, kernel_regularizer=regularizers.l2(4e-5), activation = 'relu'), # C5\n",
        "        layers.Dense(84, kernel_regularizer=regularizers.l2(4e-5), activation = 'relu'), # F6\n",
        "        layers.Dense(num_classes, activation='softmax'), # OUTPUT\n",
        "    ]\n",
        ")\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 28, 28, 6)         156       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 14, 14, 6)        0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 10, 10, 16)        2416      \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 5, 5, 16)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 400)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 120)               48120     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 84)                10164     \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 10)                850       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 61,706\n",
            "Trainable params: 61,706\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VmVzUU9oejuO"
      },
      "source": [
        "#### 2.4 LeNet-5 Model Training\n",
        "\n",
        "Train LeNet-5 model with specific training strategy. **(20 Points)**\n",
        "\n",
        "**Tasks:**\n",
        "1. Set batch size to **64** for training. \n",
        "2. Pick **SGD optimizer** with learning rate of **0.1**, momentum of **0.9**, and **nesterov=True**, for model training.\n",
        "3. Pick **cross-entropy** loss function for optimization and evaluation metrics is set to **accuracy**.\n",
        "4. Set validation_split to **0.1** which means it excludes 1/10 training data for validation process.\n",
        "4. Train the model with **10 epochs**.\n",
        "5. Evaluate model with test data set and **PRINT OUT** : **test loss** and **test accuracy**. Note that the model here is the **LAST** model after **10 epochs** training.\n",
        "\n",
        "**Hints:**\n",
        "1. You may consider the examples from Keras to specify optimizer parameters. https://keras.io/api/optimizers/\n",
        "2. You may use the example shown in class to faciliate this implementation.\n",
        "3. You may see slightly different results every time you run the training. It is normal since there is randomness for training. However, you should expect the **BEST** validation accuracy is above **87%** which may not be the result from last epoch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mz-Cvsu8ejuO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82e8c6ea-e394-4595-9cb2-067aa45b459b"
      },
      "source": [
        "## Train with SGD optimizer with learning rate =0.1, regularizer=4e-5, momentum=0.9, and nesterov=True\n",
        "\n",
        "batch_size = 64\n",
        "epochs = 10\n",
        "sgd = keras.optimizers.SGD(lr=0.1, momentum=0.9, nesterov=True)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "844/844 [==============================] - 46s 54ms/step - loss: 0.5775 - accuracy: 0.7949 - val_loss: 0.4193 - val_accuracy: 0.8467\n",
            "Epoch 2/10\n",
            "844/844 [==============================] - 41s 49ms/step - loss: 0.3795 - accuracy: 0.8679 - val_loss: 0.3548 - val_accuracy: 0.8752\n",
            "Epoch 3/10\n",
            "844/844 [==============================] - 40s 48ms/step - loss: 0.3315 - accuracy: 0.8845 - val_loss: 0.3373 - val_accuracy: 0.8802\n",
            "Epoch 4/10\n",
            "844/844 [==============================] - 38s 44ms/step - loss: 0.3030 - accuracy: 0.8931 - val_loss: 0.3234 - val_accuracy: 0.8897\n",
            "Epoch 5/10\n",
            "844/844 [==============================] - 36s 42ms/step - loss: 0.2820 - accuracy: 0.9017 - val_loss: 0.2981 - val_accuracy: 0.8950\n",
            "Epoch 6/10\n",
            "844/844 [==============================] - 35s 41ms/step - loss: 0.2668 - accuracy: 0.9077 - val_loss: 0.3269 - val_accuracy: 0.8905\n",
            "Epoch 7/10\n",
            "844/844 [==============================] - 35s 42ms/step - loss: 0.2534 - accuracy: 0.9126 - val_loss: 0.2775 - val_accuracy: 0.9057\n",
            "Epoch 8/10\n",
            "844/844 [==============================] - 37s 44ms/step - loss: 0.2416 - accuracy: 0.9164 - val_loss: 0.2842 - val_accuracy: 0.9025\n",
            "Epoch 9/10\n",
            "844/844 [==============================] - 34s 40ms/step - loss: 0.2299 - accuracy: 0.9223 - val_loss: 0.2976 - val_accuracy: 0.8992\n",
            "Epoch 10/10\n",
            "844/844 [==============================] - 34s 41ms/step - loss: 0.2224 - accuracy: 0.9241 - val_loss: 0.2927 - val_accuracy: 0.9042\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zi-xsLUiejuO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47dc8618-27dc-4dbb-9854-00847810154e"
      },
      "source": [
        "## Print out the evaluation results, including test loss and test accuracy.\n",
        "\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print(\"Test loss:\", score[0])\n",
        "print(\"Test accuracy:\", score[1])\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.30173322558403015\n",
            "Test accuracy: 0.8999999761581421\n"
          ]
        }
      ]
    }
  ]
}